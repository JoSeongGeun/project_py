import os
import json
import numpy as np
import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.utils import simple_preprocess
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity


# ğŸ“Œ í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(BASE_DIR, "data.csv")


# ğŸ“Œ CSV ì €ì¥ í•¨ìˆ˜
def save_dataframe(df, filename=DATA_FILE):
    df_copy = df.copy()
    df_copy["tagged_doc"] = df_copy["tagged_doc"].apply(json.dumps)
    df_copy["doc2vec_vector"] = df_copy["doc2vec_vector"].apply(lambda x: json.dumps(x.tolist()) if isinstance(x, np.ndarray) else json.dumps(x))
    df_copy.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"âœ… DataFrameì´ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë¨!")


# ğŸ“Œ CSV ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_dataframe(filename=DATA_FILE):
    if not os.path.exists(filename):
        print(f"âš ï¸ '{filename}' íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return create_sample_data()
    
    df = pd.read_csv(filename, encoding="utf-8-sig")
    df["tagged_doc"] = df["tagged_doc"].apply(json.loads)
    df["doc2vec_vector"] = df["doc2vec_vector"].apply(json.loads)
    df["doc2vec_vector"] = df["doc2vec_vector"].apply(lambda x: np.array(x))
    
    print(f"âœ… '{filename}' íŒŒì¼ì—ì„œ DataFrame ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!")
    return df


# ğŸ“Œ ìƒ˜í”Œ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def create_sample_data():
    sample_data = {
        "ì˜ˆì‹ì¥": ["ì›¨ë”©í™€A", "ì›¨ë”©í™€B", "ì›¨ë”©í™€C"],
        "ëŒ€ê´€ë£Œ": [1500000, 2000000, 2500000],
        "ì‹ëŒ€": [50000, 60000, 70000],
        "ìµœì†Œìˆ˜ìš©ì¸ì›": [100, 150, 200],
        "ìµœëŒ€ìˆ˜ìš©ì¸ì›": [500, 1000, 1500],
        "ì£¼ì°¨ì¥(ëŒ€)": [100, 200, 300],
        "tagged_doc": [["ì¢‹ë‹¤", "ë©‹ì§€ë‹¤"], ["ì˜ˆì˜ë‹¤", "í™”ë ¤í•˜ë‹¤"], ["ê³ ê¸‰ìŠ¤ëŸ½ë‹¤", "ì•„ë¦„ë‹µë‹¤"]],
        "doc2vec_vector": [np.random.rand(300).tolist(), np.random.rand(300).tolist(), np.random.rand(300).tolist()]
    }
    df_sample = pd.DataFrame(sample_data)
    save_dataframe(df_sample, DATA_FILE)
    return df_sample


# ğŸ“Œ ì›¨ë”©í™€ ì¶”ì²œ í•¨ìˆ˜
def recommend_wedding_hall(survey_df, df, top_n=10, review_weight=1, rental_fee_weight=0.7, food_price_weight=0.5, mini_hc_weight=0.7, limit_hc_weight=0.1, car_park_weight=0.5):

    # 1. ë¦¬ë·° ìœ ì‚¬ë„ ê³„ì‚°
    target_vector = survey_df.loc[0, "doc2vec_vector"].reshape(1, -1)
    all_vector = np.stack(df["doc2vec_vector"].values, axis=0)
    review_sim = cosine_similarity(target_vector, all_vector).flatten()

    # 2. ëŒ€ê´€ë£Œ ìœ ì‚¬ë„ ê³„ì‚°
    target_rental_fee = survey_df.loc[0, "ëŒ€ê´€ë£Œ"]
    all_rental_fee = df["ëŒ€ê´€ë£Œ"].values.reshape(1, -1)
    rental_fee_diff = np.abs(all_rental_fee - target_rental_fee)
    rental_fee_sim = 1 - MinMaxScaler().fit_transform(rental_fee_diff).flatten()

    # 3. ì‹ëŒ€ ìœ ì‚¬ë„ ê³„ì‚°
    target_food_price = survey_df.loc[0, "ì‹ëŒ€"]
    all_food_price = df["ì‹ëŒ€"].values.reshape(1, -1)
    food_price_diff = np.abs(all_food_price - target_food_price)
    food_price_sim = 1 - MinMaxScaler().fit_transform(food_price_diff).flatten()

    # 4. ìµœì†Œìˆ˜ìš©ì¸ì› í•„í„°ë§ ë° ìœ ì‚¬ë„ ê³„ì‚°
    target_mini_hc = survey_df.loc[0, "ìµœì†Œìˆ˜ìš©ì¸ì›"]
    all_mini_hc = df["ìµœì†Œìˆ˜ìš©ì¸ì›"].values
    valid_indices = np.where(all_mini_hc >= target_mini_hc)[0]

    df_filtered = df.iloc[valid_indices].reset_index(drop=True)
    review_sim = review_sim[valid_indices]
    rental_fee_sim = rental_fee_sim[valid_indices]
    food_price_sim = food_price_sim[valid_indices]
    all_mini_hc = all_mini_hc[valid_indices]

    mini_hc_diff = np.abs(all_mini_hc - target_mini_hc).reshape(-1, 1)
    mini_hc_sim = 1 - MinMaxScaler().fit_transform(mini_hc_diff).flatten()

    # 5. ìµœëŒ€ìˆ˜ìš©ì¸ì› ìœ ì‚¬ë„ ê³„ì‚°
    target_limit_hc = survey_df.loc[0, "ìµœëŒ€ìˆ˜ìš©ì¸ì›"]
    all_limit_hc = df_filtered["ìµœëŒ€ìˆ˜ìš©ì¸ì›"].values
    limit_hc_diff = np.abs(all_limit_hc - target_limit_hc)
    limit_hc_sim = 1 - MinMaxScaler().fit_transform(limit_hc_diff.reshape(-1, 1)).flatten()

    # 6. ì£¼ì°¨ì¥(ëŒ€) ìœ ì‚¬ë„ ê³„ì‚°
    target_car_park = survey_df.loc[0, "ì£¼ì°¨ì¥(ëŒ€)"]
    all_car_park = df_filtered["ì£¼ì°¨ì¥(ëŒ€)"].values
    car_park_diff = np.abs(all_car_park - target_car_park)
    car_park_sim = 1 - MinMaxScaler().fit_transform(car_park_diff.reshape(-1, 1)).flatten()

    # 7. ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
    final_sim = (
        (review_sim * review_weight) +
        (rental_fee_sim * rental_fee_weight) +
        (food_price_sim * food_price_weight) +
        (mini_hc_sim * mini_hc_weight) +
        (limit_hc_sim * limit_hc_weight) +
        (car_park_sim * car_park_weight)
    )

    final_sim[0] = -np.inf  # ìê¸° ìì‹  ì œì™¸
    top_indices = np.argsort(final_sim)[-top_n:][::-1]

    # 8. ê²°ê³¼ ë°˜í™˜
    result_df = df_filtered.iloc[top_indices].copy()
    result_df["final_similarity"] = final_sim[top_indices]

    return result_df


# ğŸ“Œ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    df = load_dataframe()

    survey = {
        "ë¦¬ë·°": [["ì¢‹ë‹¤", "ë©‹ì§€ë‹¤", "ì˜ˆì˜ë‹¤", "ì¶•ë³µ", "ê´‘ì£¼", "ê²°í˜¼ì‹"]],
        "ëŒ€ê´€ë£Œ": [2000000],
        "ì‹ëŒ€": [60000],
        "ìµœì†Œìˆ˜ìš©ì¸ì›": [150],
        "ìµœëŒ€ìˆ˜ìš©ì¸ì›": [1000],
        "ì£¼ì°¨ì¥(ëŒ€)": [1000]
    }
    survey_df = pd.DataFrame(survey)
    survey_df["doc2vec_vector"] = survey_df["ë¦¬ë·°"].apply(lambda x: np.random.rand(300))

    recommendations = recommend_wedding_hall(survey_df, df, top_n=5)

    print("\nğŸ† ì¶”ì²œ ì˜ˆì‹ì¥ ë¦¬ìŠ¤íŠ¸:")
    print(recommendations.to_string(index=False))
